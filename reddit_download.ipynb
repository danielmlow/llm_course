{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielmlow/llm_course/blob/main/reddit_download.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QDM-BZ0KVB-c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "  Using cached praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Using cached prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Using cached update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting websocket-client>=0.54.0 (from praw)\n",
            "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /Users/danielmlow/miniconda3/envs/llm_course/lib/python3.11/site-packages (from prawcore<3,>=2.4->praw) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/danielmlow/miniconda3/envs/llm_course/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/danielmlow/miniconda3/envs/llm_course/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/danielmlow/miniconda3/envs/llm_course/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/danielmlow/miniconda3/envs/llm_course/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.8.3)\n",
            "Using cached praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "Using cached prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Using cached update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
            "Installing collected packages: websocket-client, update_checker, prawcore, praw\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [praw]\n",
            "\u001b[1A\u001b[2KSuccessfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0 websocket-client-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import praw\n",
        "import pandas as pd\n",
        "from datetime import datetime, timezone\n",
        "import time\n",
        "from typing import List, Optional\n",
        "import api_keys\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_74cQVCtVB-c"
      },
      "source": [
        "Search for similar reddits here:\n",
        "https://anvaka.github.io/sayit/?query=GriefSupport"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RbNFuY_8VB-d"
      },
      "outputs": [],
      "source": [
        "sample_size = 10\n",
        "\n",
        "subreddits = ['bullying',\n",
        " 'relationship_advice',\n",
        " 'lonely',\n",
        " 'Anxiety',\n",
        " 'AskArgentina',\n",
        " 'r/BuenosAires',\n",
        " 'asktransgender'\n",
        " ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P0Mz0BapVB-d"
      },
      "outputs": [],
      "source": [
        "class RedditSampler:\n",
        "    def __init__(self, client_id: str, client_secret: str, user_agent: str):\n",
        "        self.reddit = praw.Reddit(\n",
        "            client_id=client_id,\n",
        "            client_secret=client_secret,\n",
        "            user_agent=user_agent\n",
        "        )\n",
        "\n",
        "    def get_samples(\n",
        "        self,\n",
        "        subreddits: List[str],\n",
        "        sample_size: int = 6000,\n",
        "        sleep_amount: float = 0.1,\n",
        "        submission_type: str = \"all\"\n",
        "    ) -> pd.DataFrame:\n",
        "        all_submissions = []\n",
        "        sorts = ['new', 'top', 'controversial', 'hot']\n",
        "        time_filters = ['all', 'year', 'month', 'week']\n",
        "\n",
        "        for subreddit_name in subreddits:\n",
        "            try:\n",
        "                subreddit = self.reddit.subreddit(subreddit_name)\n",
        "                # Use a dictionary with submission ID as key for deduplication\n",
        "                submission_dict = {}\n",
        "\n",
        "                for sort in sorts:\n",
        "                    if len(submission_dict) >= sample_size:\n",
        "                        break\n",
        "\n",
        "                    for time_filter in time_filters:\n",
        "                        if len(submission_dict) >= sample_size:\n",
        "                            break\n",
        "\n",
        "                        try:\n",
        "                            if sort == 'new':\n",
        "                                submissions = subreddit.new(limit=1000)\n",
        "                            elif sort == 'hot':\n",
        "                                submissions = subreddit.hot(limit=1000)\n",
        "                            else:\n",
        "                                submissions = getattr(subreddit, sort)(time_filter=time_filter, limit=1000)\n",
        "\n",
        "                            for submission in submissions:\n",
        "                                if len(submission_dict) >= sample_size:\n",
        "                                    break\n",
        "\n",
        "                                if submission_type != \"all\":\n",
        "                                    if submission_type == \"self\" and not submission.is_self:\n",
        "                                        continue\n",
        "                                    if submission_type == \"link\" and submission.is_self:\n",
        "                                        continue\n",
        "\n",
        "                                # Skip if we've already processed this submission ID\n",
        "                                if submission.id in submission_dict:\n",
        "                                    continue\n",
        "\n",
        "                                sub_dict = {\n",
        "                                    'subreddit': subreddit_name,\n",
        "                                    'id': submission.id,\n",
        "                                    'title': submission.title,\n",
        "                                    'author': str(submission.author),\n",
        "                                    'created_utc': datetime.fromtimestamp(submission.created_utc),\n",
        "                                    'score': submission.score,\n",
        "                                    'upvote_ratio': submission.upvote_ratio,\n",
        "                                    'num_comments': submission.num_comments,\n",
        "                                    'url': submission.url,\n",
        "                                    'is_self': submission.is_self,\n",
        "                                    'selftext': submission.selftext if submission.is_self else None,\n",
        "                                    # Store these for informational purposes\n",
        "                                    'collection_sort': sort,\n",
        "                                    'collection_time_filter': time_filter if sort != 'new' and sort != 'hot' else None\n",
        "                                }\n",
        "\n",
        "                                # Use the ID as the dictionary key for guaranteed uniqueness\n",
        "                                submission_dict[submission.id] = sub_dict\n",
        "                                time.sleep(sleep_amount)\n",
        "\n",
        "                            print(f\"Collected {len(submission_dict)} unique samples from r/{subreddit_name} ({sort}/{time_filter})\")\n",
        "                            time.sleep(1)\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error with {sort}/{time_filter}: {str(e)}\")\n",
        "                            continue\n",
        "\n",
        "                # Convert dictionary to list of values\n",
        "                all_submissions.extend(list(submission_dict.values())[:sample_size])\n",
        "                print(f\"Added {min(len(submission_dict), sample_size)} unique submissions from r/{subreddit_name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error collecting from r/{subreddit_name}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return pd.DataFrame(all_submissions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvG82h8zJtor"
      },
      "source": [
        "To get a Reddit API client ID and secret:\n",
        "\n",
        "1. Visit https://www.reddit.com/prefs/apps\n",
        "2. Sign in to your Reddit account\n",
        "3. Click \"create app\" or \"create another app\" button\n",
        "4. Fill out the form:\n",
        "   - Name: your app name\n",
        "   - Select \"web app\" or \"script\" depending on your needs\n",
        "   - Description: brief description of your app\n",
        "   - About URL: optional website URL\n",
        "   - Redirect URI: use http://localhost:8000 for testing\n",
        "5. Click \"create app\"\n",
        "\n",
        "After creation, you'll see:\n",
        "- Client ID: under your app name\n",
        "- Client secret: displayed as \"secret\"\n",
        "\n",
        "Note that Reddit has significantly restricted API access with new usage limitations and pricing since April 2023, which may affect your development plans.match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObRqrgTlVB-e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # You'll need to get these from your Reddit API application\n",
        "    CLIENT_ID = 'YOUR_CLIENT_ID' # Put your client ID in between \"\"\n",
        "    CLIENT_SECRET = \"YOUR_CLIENT_SECRET\" # Put your client secret in between \"\"\n",
        "    USER_AGENT = f\"script:data_sampler:v1.0 (by /u/{\"YOUR_USERNAME\"})\" # Put your username in between \"\"\n",
        "\n",
        "    sampler = RedditSampler(CLIENT_ID, CLIENT_SECRET, USER_AGENT)\n",
        "\n",
        "\n",
        "\n",
        "    # Get samples\n",
        "    samples_df = sampler.get_samples(\n",
        "        subreddits=subreddits,\n",
        "        sample_size=sample_size,\n",
        "        submission_type=\"all\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxKANLxWVB-e"
      },
      "outputs": [],
      "source": [
        "samples_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gO25_S0lchQD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Any duplicates?\n",
        "samples_df['id'].value_counts().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OauOIcjVB-e"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "# now\n",
        "now = datetime.now()\n",
        "format = '%y-%m-%dT%H-%M-%S'\n",
        "date_string = now.strftime(format)\n",
        "date_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyIX5cfuVB-e"
      },
      "outputs": [],
      "source": [
        "samples_df['subreddit'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sju6lYWjVB-e"
      },
      "outputs": [],
      "source": [
        "samples_df['title_text'] = samples_df['title']+\"\\n---\\n\"+samples_df['selftext']\n",
        "\n",
        "# Save to CSV\n",
        "samples_df.to_csv(f\"reddit_{date_string}_incomplete.csv\", index=False)\n",
        "print(f\"Saved {len(samples_df)} total samples to reddit_samples.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqxBLIOtVB-e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm_course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
